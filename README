FOR OBJECT TRAJECTORY
=========================
vrProjectorWrapper.py (folder vrProjector):
----------------------
python vrProjectorWrapper.py --sourceVideo <<video name>> --outWidth 1024 --outHeight 1024

StitchingFrames.py (folder YOLO):
------------------
python StitchingFrames.py --outWidth 1024 --outHeight 1024 --dirPath <<path to directory with all frames, use / at end here>>

StitchedObjectDetectionWrapper.py (folder YOLO):
---------------------------------
python StitchedObjectDetectionWrapper.py --source <<source directory with all stitched frames>> --metafilename <<output file name with object bounding boxes>>

StitchedBoundingBoxConverter.py (folder vrProjector):
-------------------------------
python StitchedBoundingBoxConverter.py --sourceMetaFilePath <<above metadata file name>> --outWidth 1024 --outHeight 1024 --dirPath <<directory name with original frames, don't use / at end here>>




FOR VIEWPORT TRAJECTORY
==========================
1. Go to 'GET\ VIEWPORT' directory
2. Make sure there is `output$num.csv` file that contains the gyroscope data for a video
3. Set `usernum=$num` in 'getviewport.py' file
4. `python getviewport.py`



FOR PREDICTION
================
1. Get viewport trajectory(from 'GET VIEWPORT' folder) and object trajectory(from 'YOLO' folder[filename = lovish_stitched_metadata-equirectangular-object-info.npy]) in the directory
2. Set `usernum=$num` corresponding to the user needed
3. `python predictionmodel_new.py`(for next frame only)
4. `python predframes_new.py`(for next 30 frames; change number of frames to predict by changing `pred_nframes`)
